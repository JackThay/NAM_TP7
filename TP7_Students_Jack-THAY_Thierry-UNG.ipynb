{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab7 (Students version): PageRank\n",
    "## Students:\n",
    "## Jack Thay - 21201079\n",
    "## Thierry Ung - 3804472\n",
    "We can use the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import time\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: preliminary questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab work, we use the graph Wiki that you will find on Moodle. It is a subpart of the English language Wikipedia. A link represent a hyperlink from a page to another.\n",
    "\n",
    "**Warning:** it is a directed graph, so in this case a line\n",
    "\n",
    "12 126\n",
    "\n",
    "means that there is a directed link from node 12 to node 126, but not necessarily in the other direction!\n",
    "\n",
    "For your information, we indicate that this network has:\n",
    "\n",
    "- 50988 nodes\n",
    "\n",
    "- 1778125 directed links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: preliminaries\n",
    "\n",
    "Load the graph in memory, in the adjacency list format, **for both the list of predecessors and the list of successors**. \n",
    "\n",
    "Check its number of nodes and of directed links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from our TP1\n",
    "\n",
    "def load_directed_graph(filename):\n",
    "    # Initialize empty dictionaries for the forward and reverse adjacency lists.\n",
    "    adjacency_list = {}\n",
    "    reverse_adjacency_list = {}\n",
    "\n",
    "    # Open the specified file for reading.\n",
    "    with open(filename, 'r') as file:\n",
    "        # Loop through each line in the file.\n",
    "        for line in file:\n",
    "            # Check if the line is not empty.\n",
    "            if line.strip():\n",
    "                # Split the line into two nodes representing an edge.\n",
    "                edge_info = line.strip().split()\n",
    "                node1, node2 = edge_info[0], edge_info[1]\n",
    "                # Check if node1 is not already in the forward adjacency list.\n",
    "                if node1 not in adjacency_list:\n",
    "                    # Initialize an empty list for node1 in both forward and reverse lists.\n",
    "                    adjacency_list[node1] = []\n",
    "                    reverse_adjacency_list[node1] = []\n",
    "                # Check if node2 is not already in the forward adjacency list.\n",
    "                if node2 not in adjacency_list:\n",
    "                    # Initialize an empty list for node2 in both forward and reverse lists.\n",
    "                    adjacency_list[node2] = []\n",
    "                    reverse_adjacency_list[node2] = []\n",
    "                # Add node2 to the adjacency list of node1 (forward edge).\n",
    "                adjacency_list[node1].append(node2)\n",
    "                # Add node1 to the reverse adjacency list of node2 (reverse edge).\n",
    "                reverse_adjacency_list[node2].append(node1)\n",
    "\n",
    "    # Return the two adjacency lists.\n",
    "    return adjacency_list, reverse_adjacency_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_node(graph):\n",
    "    return len(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_link(graph):\n",
    "    count = 0\n",
    "    for node in graph:\n",
    "        count += len(graph[node])\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"wiki_2009.txt\"\n",
    "adjacency_list, reverse_adjacency_list = load_directed_graph(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_node(adjacency_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_node(reverse_adjacency_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_link(adjacency_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_link(reverse_adjacency_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remind you that the transition matrix $T$ is defined like this: if there is a link $ u \\rightarrow v$ then $T[v][u] = \\frac{1}{d_{out}(u)}$ where $d_{out}(u)$ is the out-degree of $u$ and otherwise  $T[v][u] = 0$.\n",
    "\n",
    "Note that it is not possible to store $T$ in memory as a $ n \\times n $ matrix, it would take too much memory. So instead of explicitly computing a matrix $T$, we use the adjacency lists of the graph (lists of predecessors, lists of successors) in this way: \n",
    "\n",
    "- from the list of successors, we store the outdegree ($ d_{out}$) of the nodes in a dedicated dictionary,\n",
    "\n",
    "- we use that a node $v$ receive PageRank from node $u$ if and only if $u$ is a predecessor of $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: out-degree and dead-ends\n",
    "\n",
    "Create a dictionary that contains the out-degree of each node. Note that to avoid problems later, you should give the out-degree of all the nodes if the network, even if it is $0$. Report the fraction of nodes which are dead-ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from our TP1\n",
    "\n",
    "def compute_degree_distribution_directed(adjacency_list, reverse_adjacency_list):\n",
    "    # Initialize two empty dictionaries to store degree distributions.\n",
    "    in_degree_distribution = {}   # For in-degrees\n",
    "    out_degree_distribution = {}  # For out-degrees\n",
    "\n",
    "    # Compute out-degree distribution.\n",
    "    for node, out_neighbors in adjacency_list.items():\n",
    "        # Calculate the out-degree of the current node by finding the number of outgoing edges.\n",
    "        out_degree = len(out_neighbors)\n",
    "        # Check if the out-degree already exists as a key in the out_degree_distribution dictionary.\n",
    "        if out_degree in out_degree_distribution:\n",
    "            # If it exists, increment the count for that out-degree by 1.\n",
    "            out_degree_distribution[out_degree] += 1\n",
    "        else:\n",
    "            # If it doesn't exist, add it to the out_degree_distribution dictionary with a count of 1.\n",
    "            out_degree_distribution[out_degree] = 1\n",
    "\n",
    "    # Compute in-degree distribution.\n",
    "    for node, in_neighbors in reverse_adjacency_list.items():\n",
    "        # Calculate the in-degree of the current node by finding the number of incoming edges.\n",
    "        in_degree = len(in_neighbors)\n",
    "        # Check if the in-degree already exists as a key in the in_degree_distribution dictionary.\n",
    "        if in_degree in in_degree_distribution:\n",
    "            # If it exists, increment the count for that in-degree by 1.\n",
    "            in_degree_distribution[in_degree] += 1\n",
    "        else:\n",
    "            # If it doesn't exist, add it to the in_degree_distribution dictionary with a count of 1.\n",
    "            in_degree_distribution[in_degree] = 1\n",
    "\n",
    "    # Return both in-degree and out-degree distributions as dictionaries.\n",
    "    return in_degree_distribution, out_degree_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degree_dist, out_degree_dist = compute_degree_distribution_directed(adjacency_list, reverse_adjacency_list)\n",
    "print(\"\\nOut-Degree Distribution:\")\n",
    "print(out_degree_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: PageRank without evaporation\n",
    "\n",
    "Using the previous questions, implement a basic power iteration (for the moment, it is not a complete PageRank algorithm). \n",
    "\n",
    "The principle is to iterate $t$ times the matrix product:\n",
    "\n",
    "$$ X \\leftarrow T.X $$\n",
    "\n",
    "$X$ is a vector initialized to $ [\\frac{1}{n} \\ldots \\frac{1}{n}]$ and $T$ is the transition matrix. We strongly advise you to store $X$ as a dictionary of float.\n",
    "\n",
    "Run the power iteration for $ t=10 $ steps and measure at each step the value of $ ||X||_1 = \\sum _i |X[i]| $.\n",
    "What do you observe? Can you explain in one sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vector(nodes):\n",
    "    n = len(nodes)\n",
    "    # Initialize the vector X with equal probabilities for all nodes.\n",
    "    initial_value = 1 / n\n",
    "    vector = {node: initial_value for node in nodes}\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_iteration(adjacency_list, iterations=10):\n",
    "    # Get the list of nodes from the adjacency list.\n",
    "    nodes = list(adjacency_list.keys())\n",
    "\n",
    "    # Initialize the vector X.\n",
    "    vector_X = initialize_vector(nodes)\n",
    "\n",
    "    # Perform power iteration for the specified number of iterations.\n",
    "    for iteration in range(iterations):\n",
    "        # Initialize a new vector to store the result of matrix multiplication T.X.\n",
    "        new_vector_X = {node: 0.0 for node in nodes}\n",
    "\n",
    "        # Iterate over each node in the graph.\n",
    "        for node in nodes:\n",
    "            # Get the out-neighbors of the current node.\n",
    "            out_neighbors = adjacency_list[node]\n",
    "            # Calculate the contribution of each out-neighbor to the new vector.\n",
    "            for out_neighbor in out_neighbors:\n",
    "                new_vector_X[out_neighbor] += vector_X[node] / len(out_neighbors)\n",
    "\n",
    "        # Update the vector X with the result of matrix multiplication.\n",
    "        vector_X = new_vector_X\n",
    "\n",
    "        # Calculate and print the value of ||X||_1 at each step.\n",
    "        norm_value = sum(abs(value) for value in vector_X.values())\n",
    "        print(f\"Iteration {iteration + 1}, ||X||_1: {norm_value}\")\n",
    "\n",
    "    return vector_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_vector = power_iteration(reverse_adjacency_list, iterations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:  \n",
    "The L1 norm $ ||X||_1 $ of the vector X consistently decreases over the iterations, indicating a potential concentration of probability mass on fewer nodes or a more even distribution across the nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Pagerank with evaporation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: implementing a complete PageRank\n",
    "\n",
    "Now, we improve the basic power iteration process to make a real Pagerank program, by adding a normalization and an evaporation (or teleportation) process. So now each iteration is described by:\n",
    "\n",
    "$$ X \\leftarrow (1-s).T.X + s.I $$\n",
    "\n",
    "where $I$ is the vector $ [\\frac{1}{n} \\ldots \\frac{1}{n}]$.\n",
    "\n",
    "* Implement the Pagerank algorithm (as in the course). Don't forget to normalize $X$ after each step, that is to say do: $ X[i] \\leftarrow \\frac{X[i]}{||X||_1}$.\n",
    "\n",
    "* Run the Pagerank for $t=10$ steps, $s=0.2$.\n",
    "\n",
    "* Observe the nodes which have the top-5 pagerank values, then go and have a look to the Wiki index also provided on Moodle, to see to what Wikipedia pages they correspond. \n",
    "\n",
    "* Comment your results: are you surprised by the top-ranked pages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_pagerank(adjacency_list, iterations=10, damping_factor=0.2):\n",
    "    nodes = list(adjacency_list.keys())\n",
    "    n = len(nodes)\n",
    "\n",
    "    # Initialize the vector X with equal probabilities for all nodes.\n",
    "    initial_value = 1 / n\n",
    "    vector_X = {node: initial_value for node in nodes}\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        # Initialize a new vector to store the result of matrix multiplication (1-s)T.X + sI.\n",
    "        new_vector_X = {node: 0.0 for node in nodes}\n",
    "\n",
    "        # Teleportation: Add the teleportation factor s * I to the new vector.\n",
    "        for node in nodes:\n",
    "            new_vector_X[node] += damping_factor / n\n",
    "\n",
    "        # Matrix multiplication: (1-s)T.X\n",
    "        for node in nodes:\n",
    "            out_neighbors = adjacency_list[node]\n",
    "            for out_neighbor in out_neighbors:\n",
    "                new_vector_X[out_neighbor] += (1 - damping_factor) * vector_X[node] / len(out_neighbors)\n",
    "\n",
    "        # Update the vector X with the result of matrix multiplication.\n",
    "        vector_X = new_vector_X\n",
    "\n",
    "        # Normalize the vector X.\n",
    "        norm_value = sum(abs(value) for value in vector_X.values())\n",
    "        vector_X = {node: value / norm_value for node, value in vector_X.items()}\n",
    "\n",
    "        # Print the value of ||X||_1 at each step.\n",
    "        print(f\"Iteration {iteration + 1}, ||X||_1: {norm_value}\")\n",
    "\n",
    "    return vector_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_result_vector = complete_pagerank(reverse_adjacency_list, iterations=10, damping_factor=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wiki_index(filename):\n",
    "    wiki_index = {}\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            # Skip lines that don't contain valid page information or start with '#'\n",
    "            if not line.strip() or line.startswith('#'):\n",
    "                continue\n",
    "            # Split the line into page ID and page title (handle the case of missing title)\n",
    "            parts = line.strip().split('\\t', 1)\n",
    "            if len(parts) == 2:\n",
    "                page_id, page_title = parts\n",
    "            else:\n",
    "                # Skip lines with missing page ID or invalid format\n",
    "                continue\n",
    "            # Convert page ID to integer\n",
    "            try:\n",
    "                page_id = int(page_id)\n",
    "            except ValueError:\n",
    "                # Skip lines with non-integer page ID\n",
    "                continue\n",
    "            # Add entry to the dictionary\n",
    "            wiki_index[page_id] = page_title\n",
    "\n",
    "    return wiki_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming result_vector is the output of your pagerank function\n",
    "sorted_nodes = sorted(complete_result_vector.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top-5 nodes with their PageRank scores\n",
    "top_5_nodes = [node for node, _ in sorted_nodes[:5]]\n",
    "print(\"Top-5 Pages:\")\n",
    "for node in top_5_nodes:\n",
    "    print(f\"{node}: {complete_result_vector[node]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wiki index\n",
    "wiki_index_filename = \"index_wiki_2009.txt\"\n",
    "wiki_index = load_wiki_index(wiki_index_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert string keys to integers in title_to_id_mapping\n",
    "title_to_id_mapping = {title: node for node, title in wiki_index.items()}\n",
    "title_to_id_mapping = {int(title) if str(title).isdigit() else int(wiki_index[title]): title for title in title_to_id_mapping.values()}\n",
    "\n",
    "# Convert the IDs in top_5_nodes to integers\n",
    "top_5_nodes = [int(node) for node in top_5_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Corresponding Wikipedia Pages:\")\n",
    "for node in top_5_nodes:\n",
    "    # Convert the PageRank ID to the corresponding page title\n",
    "    if node in title_to_id_mapping:\n",
    "        corresponding_title = wiki_index[title_to_id_mapping[node]]\n",
    "        print(f\"{node}: {corresponding_title}\")\n",
    "    else:\n",
    "        print(f\"{node}: No corresponding Wikipedia page found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer : \n",
    "Some of the results of the top-ranked pages from the PageRank analysis are strange:\n",
    "\n",
    "\n",
    "- \"Federal Information Processing Standard state code\" stands out as it is really technical, being in the top 5 ranked page make it look like something went wrong, unless americans search for this on a regular basis without our knowledge.  \n",
    "- \"National Film Registry\" may make sense if it's the american film registry, but we're having doubts it was read that often back then.\n",
    "\n",
    "The other 3 seems to make sense:\n",
    "\n",
    "- \"List of Biblical names\" is relevant since it take into account jewish, christian names and the wide influence of the Bible in our culture.  \n",
    "- \"History of present-day nations and states\" makes sense as a broadly important topic.  \n",
    "- \"List of deities\" is also understandable as a widely referenced topic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: correlation with other centrality scores\n",
    "For the first thousand nodes ranked, plot their PageRank as a function of their in-degree. What do you observe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the PageRank for the first thousand nodes\n",
    "result_vector = complete_pagerank(adjacency_list, iterations=10, damping_factor=0.2)\n",
    "sorted_nodes = sorted(result_vector.items(), key=lambda x: x[1], reverse=True)\n",
    "top_1000_nodes = [node for node, _ in sorted_nodes[:1000]]\n",
    "\n",
    "# Extract in-degrees for the top 1000 nodes\n",
    "in_degrees = {node: len(reverse_adjacency_list[node]) for node in top_1000_nodes}\n",
    "\n",
    "# Extract PageRanks for the top 1000 nodes\n",
    "pageranks_top = [result_vector[node] for node in top_1000_nodes]\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(in_degrees.values(), pageranks_top, alpha=0.5)\n",
    "plt.title('PageRank : In-Degree for the First 1000 Nodes')\n",
    "plt.xlabel('In-Degree')\n",
    "plt.ylabel('PageRank')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer :\n",
    "\n",
    "There is a strong positive correlation between PageRank and in-degree for the first 1000 nodes. This means the nodes with the highest in-degree (the one with the most incoming links) tend to have the highest PageRank.\n",
    "\n",
    "Probably because PageRank is calculated based on the quantity of incoming links. A node with a high in-degree is more likely to be important because it is linked to by many other nodes, which will most likely include other important nodes.\n",
    "\n",
    "However, there are also some nodes with a high in-degree but a relatively low PageRank. This can happen if the incoming links are from low-quality nodes (presumably, pages with a low quantity of information). For example, a node may have a lot of incoming links from spam websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Convergence of the PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: measuring the convergence speed\n",
    "\n",
    "Instead of setting the number of steps, we want to give another stopping criterion: the process is stopped when the norm of the difference of the PageRank varies by less than $\\epsilon$ between two steps. In other words, the algorithm stops when\n",
    "\n",
    "$$    \\sum _i |X_t[i] - X_{t-1}[i]|  < \\epsilon $$\n",
    "\n",
    "Modify the algorithm to adapt to this criterion and then measure the number of iterations needed with an $ \\epsilon = 10^{-5}$ (with $ \\alpha = 0.2 $, as before).\n",
    "\n",
    "Plot $  \\sum _i |X_t[i] - X_{t-1}[i]| $ as a function of $t$, use a logscale for the $y$ axis, what do you observe? (we don't ask you to explain this observation).\n",
    "\n",
    "What you should observe here is one of the key of the PageRank success: its very fast convergence on real-world networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank_convergence(adjacency_list, epsilon=1e-5, damping_factor=0.2):\n",
    "    nodes = list(adjacency_list.keys())\n",
    "    n = len(nodes)\n",
    "\n",
    "    # Initialize the vector X with equal probabilities for all nodes.\n",
    "    initial_value = 1 / n\n",
    "    vector_X = {node: initial_value for node in nodes}\n",
    "\n",
    "    norm_differences = []  # List to store norm differences at each iteration.\n",
    "    iteration = 0\n",
    "\n",
    "    while True:\n",
    "        # Initialize a new vector to store the result of matrix multiplication (1-s)T.X + sI.\n",
    "        new_vector_X = {node: 0.0 for node in nodes}\n",
    "\n",
    "        # Teleportation: Add the teleportation factor s * I to the new vector.\n",
    "        for node in nodes:\n",
    "            new_vector_X[node] += damping_factor / n\n",
    "\n",
    "        # Matrix multiplication: (1-s)T.X\n",
    "        for node in nodes:\n",
    "            out_neighbors = adjacency_list[node]\n",
    "            for out_neighbor in out_neighbors:\n",
    "                new_vector_X[out_neighbor] += (1 - damping_factor) * vector_X[node] / len(out_neighbors)\n",
    "\n",
    "        # Update the vector X with the result of matrix multiplication.\n",
    "        new_norm_value = sum(abs(value) for value in new_vector_X.values())\n",
    "        new_vector_X = {node: value / new_norm_value for node, value in new_vector_X.items()}\n",
    "\n",
    "        # Calculate the norm of the difference between the new and old vectors.\n",
    "        norm_difference = sum(abs(new_vector_X[node] - vector_X[node]) for node in nodes)\n",
    "        norm_differences.append(norm_difference)\n",
    "\n",
    "        # Update the vector X and iteration count.\n",
    "        vector_X = new_vector_X\n",
    "        iteration += 1\n",
    "\n",
    "        # Print the value of ||X||_1 at each step.\n",
    "        print(f\"Iteration {iteration}, ||X||_1: {new_norm_value}\")\n",
    "\n",
    "        # Check the stopping criterion.\n",
    "        if norm_difference < epsilon:\n",
    "            break\n",
    "\n",
    "    return vector_X, norm_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank_vector, norm_differences = pagerank_convergence(adjacency_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = range(1, len(norm_differences) + 1)\n",
    "plt.plot(iterations, norm_differences, marker='o')\n",
    "plt.yscale('log')  # Use log scale for the y-axis\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('||X_t - X_{t-1}||_1')\n",
    "plt.title('PageRank convergence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer : \n",
    "The graph shows that pagerank convergence increases as the number of iterations increases.     \n",
    "This suggests the algorithm is able to be more accurate the more iterations are made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: investigating the convergence with parameter $ \\alpha $\n",
    "\n",
    "Now, implement the algorithm with different $ \\alpha $ values, for instance $0.01$, $0.02$, $0.1$, $0.2$, $0.5$ and measure the number of iterations needed with an $ \\epsilon = 10^{-5}$. What is the effect of $ \\alpha $ on the convergence speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1e-5\n",
    "damping_factor=0.01\n",
    "pagerank_vector, norm_differences = pagerank_convergence(adjacency_list, epsilon, damping_factor)\n",
    "\n",
    "iterations = range(1, len(norm_differences) + 1)\n",
    "plt.plot(iterations, norm_differences, marker='o')\n",
    "plt.yscale('log')  # Use log scale for the y-axis\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('||X_t - X_{t-1}||_1')\n",
    "plt.title('PageRank convergence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1e-5\n",
    "damping_factor=0.02\n",
    "pagerank_vector, norm_differences = pagerank_convergence(adjacency_list, epsilon, damping_factor)\n",
    "\n",
    "iterations = range(1, len(norm_differences) + 1)\n",
    "plt.plot(iterations, norm_differences, marker='o')\n",
    "plt.yscale('log')  # Use log scale for the y-axis\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('||X_t - X_{t-1}||_1')\n",
    "plt.title('PageRank convergence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1e-5\n",
    "damping_factor=0.1\n",
    "pagerank_vector, norm_differences = pagerank_convergence(adjacency_list, epsilon, damping_factor)\n",
    "\n",
    "iterations = range(1, len(norm_differences) + 1)\n",
    "plt.plot(iterations, norm_differences, marker='o')\n",
    "plt.yscale('log')  # Use log scale for the y-axis\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('||X_t - X_{t-1}||_1')\n",
    "plt.title('PageRank convergence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1e-5\n",
    "damping_factor=0.2\n",
    "pagerank_vector, norm_differences = pagerank_convergence(adjacency_list, epsilon, damping_factor)\n",
    "\n",
    "iterations = range(1, len(norm_differences) + 1)\n",
    "plt.plot(iterations, norm_differences, marker='o')\n",
    "plt.yscale('log')  # Use log scale for the y-axis\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('||X_t - X_{t-1}||_1')\n",
    "plt.title('PageRank convergence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1e-5\n",
    "damping_factor=0.5\n",
    "pagerank_vector, norm_differences = pagerank_convergence(adjacency_list, epsilon, damping_factor)\n",
    "\n",
    "iterations = range(1, len(norm_differences) + 1)\n",
    "plt.plot(iterations, norm_differences, marker='o')\n",
    "plt.yscale('log')  # Use log scale for the y-axis\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('||X_t - X_{t-1}||_1')\n",
    "plt.title('PageRank convergence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer :\n",
    "\n",
    "A larger 𝛼 generally means a faster convergence, and a smaller 𝛼 may result in slower convergence due to the increased influence of random teleportation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
